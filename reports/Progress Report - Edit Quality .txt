1. Of all the tasks that were available ( drafttopic , article topic , edit quality and article quality ) , I was asked not to take topicing tasks ( or enhancing them ) as they already had a neural network architecture ( as work done by Issac Johnson) . So I took on the task of edit quality.
2. The task had a lot of challenges and then some eventual roadblocks that need to be cleared . 
3. First of them , being the lack of systematic extraction . The edits are rendered as a form of a table with a variety of text; it sometimes contains text , sometimes a table within a table , sometimes hypertext , sometimes undefined classes within a span with unknown attributes . So this makes parsing the data really difficult . 
4. Imbalanced data , for the labels . So the English data has almost 19500 labelled as non- damaging while only close to 700 as damaging . Other languages which I tried to fit in the language agnostic bandwidth ( the Indian Languages ) didn't have the complete set of human annotated versions . If the files had some revisions , it was not labelled . 
5. The data created a lot of problems to train or to create a model , so I created a dummy dataset of one sentence in each language to test and figure out a way to generate a hierarchy based model on gnn . The model is working but needs a data plugin . Which is really difficult to extract in terms of edit quality . Hence we need to clear the roadblock before thinking of a language agnostic model .
6. Another question which arises is what about m-bert? Why not use it . So as mentioned in the muril paper , It was clear that mbert doesn’t perform the best in language-agnostic representations in Indian languages .
7. Then I also read about labse . Which is again regarding sentence embedding in Indian Languages.
8. Given the issues , I have developed a model - The revision R is passed to XLM-Roberta and the output from its last layer is used as input feature representation of each sub-word vector 𝑈 = [𝑢1, 𝑢2, . . . , 𝑢𝑛]. Input representation 𝑈 is then fed to a 𝐾-layer attentional bidirectional GRU to get enhanced contextual representation, and the output
vectors from both directions are concatenated.
9. In this model , despite here only being two labels - I implemented the model to find the correlation between the labels . Which can be extended  to articletopic and drafttopic where the number of layers are more . 
10. The other experiments that are planned are to consider just the named entities ( that is to eliminate the hurdle required to parse the data ) using stanza . 
11. The next task will be to perform all existing ideas on articlequality as that will be much easier to perform given that the data will have uniformity . ( The idea will be to take the title and the introductory paragraph of a wikiarticle [given it is not empty ] )